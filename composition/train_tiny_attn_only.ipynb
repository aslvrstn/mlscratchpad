{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_M9TL-8GxfH",
        "outputId": "714d6bb2-977c-4b33-b21a-dea6d6fff0eb"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/aslvrstn/Easy-Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xPy-W7u8F_4b"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "\n",
        "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",
        "from easy_transformer.utils import FactoredMatrix, composition_scores\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch as t\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import random\n",
        "import wandb\n",
        "import time\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "cfg = EasyTransformerConfig(\n",
        "    d_model=64,\n",
        "    d_vocab=1000,\n",
        "    n_ctx=100,\n",
        "    n_layers=2,\n",
        "    n_heads=2,\n",
        "    d_head=32,\n",
        "    attn_only=True,\n",
        "    positional_embedding_type=\"shortformer\"  # May want to enable this\n",
        ")\n",
        "train_cfg = {\n",
        "    \"lr\": 1e-2,\n",
        "    \"batch_size\": 1000,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK-vvHYnGXwj",
        "outputId": "566f518f-fced-4633-a357-826e5546db6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09142827987670898\n",
            "tensor([[   0, 8214, 5246,  ..., 8189, 3264, 3015],\n",
            "        [ 746, 7348, 3088,  ..., 9004, 8327, 4311],\n",
            "        [   0,  126, 5258,  ..., 7750,  621, 1098],\n",
            "        ...,\n",
            "        [8637, 6540, 6925,  ..., 5080, 1965, 9479],\n",
            "        [ 464, 7718, 4185,  ..., 5196, 5297, 4981],\n",
            "        [1676,    0, 5469,  ..., 9518, 1688,  743]])\n"
          ]
        }
      ],
      "source": [
        "def generate_data(batch_size: int, seq_len: int, vocab_size: int) -> t.Tensor:\n",
        "    \"\"\"Returns (batch_size, seq_len) tensor of tokens and same-shaped 'is this part of a repeated bit'\"\"\"\n",
        "    i = 0\n",
        "    tokens_needed = batch_size * seq_len\n",
        "    tokens = t.randint(1, vocab_size, (tokens_needed,))\n",
        "    token_copy = tokens.clone()  # So we can copy from it into the same memory space\n",
        "    # learnable = t.zeros_like(tokens).bool()\n",
        "    while i < tokens_needed:\n",
        "        # Generate sentences up to half our context length (don't be cruel)\n",
        "        sent_len = random.randint(1, seq_len // 2)\n",
        "        gap_len = random.randint(0, seq_len // 2)\n",
        "\n",
        "        # Start of sequence token\n",
        "        tokens[i] = 0\n",
        "        i += 1\n",
        "\n",
        "        start_of_copy_from = i\n",
        "        end_of_copy_from = i + sent_len + 1\n",
        "        start_of_copy_to = end_of_copy_from + gap_len\n",
        "        end_of_copy_to = start_of_copy_to + sent_len + 1\n",
        "        # If we've run off the end, nothing else is learnable, and call it a day\n",
        "        if end_of_copy_to >= tokens_needed:\n",
        "            # learnable[i:] = False\n",
        "            break\n",
        "\n",
        "        tokens[start_of_copy_to:end_of_copy_to] = token_copy[start_of_copy_from:end_of_copy_from]\n",
        "        # learnable[start_of_copy_to:end_of_copy_to] = True\n",
        "        i = end_of_copy_to\n",
        "    return rearrange(tokens, \"(b s) -> b s\", b=batch_size)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "foo = generate_data(2000, 100, 10000)\n",
        "print(time.time() - start)\n",
        "print(foo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7vdQBpNiGd25"
      },
      "outputs": [],
      "source": [
        "def get_test_loss(model: t.nn.Module, test_batch_size: int) -> float:\n",
        "    with t.no_grad():\n",
        "        test_sentences = []\n",
        "        learnable_list = []\n",
        "        for sent_i in range(test_batch_size):\n",
        "            test_len = random.randint(1, cfg.n_ctx // 2 - 3)\n",
        "            test_seq = t.randint(1, cfg.d_vocab, (test_len,)).repeat(2).to(device)\n",
        "            test_seq = t.cat((t.tensor([0]).to(device), test_seq)).to(device)\n",
        "            learnable = t.cat(\n",
        "                (t.tensor([False]), t.zeros((test_len,)), t.tensor([False]), t.ones((test_len - 1,)))\n",
        "            ).bool()\n",
        "            test_sentences.append(test_seq)\n",
        "            learnable_list.append(learnable)\n",
        "\n",
        "        test_input = t.nn.utils.rnn.pad_sequence(test_sentences, batch_first=True)\n",
        "        learnable = t.nn.utils.rnn.pad_sequence(learnable_list, batch_first=True, padding_value=False)\n",
        "        test_out = model(test_input)\n",
        "\n",
        "        learnable_in = test_input[learnable]\n",
        "        learnable_out = test_out[learnable]\n",
        "        loss = F.cross_entropy(learnable_out[:-1], learnable_in[1:])\n",
        "        # print(\"LEARNABLE\")\n",
        "        # print(learnable_out[:-1].argmax(-1)[:100])\n",
        "        # print(learnable_in[1:101])\n",
        "        # print(learnable_out[:-1].argmax(-1)[:100] == learnable_in[1:101])\n",
        "\n",
        "        # def color_when_true(arr):\n",
        "        #     i = 0\n",
        "\n",
        "        #     def _inner(x):\n",
        "        #         nonlocal i\n",
        "        #         color = arr.flatten()[i]\n",
        "        #         i += 1\n",
        "        #         return f\"\\u001b[31m{x}\\u001b[0m\" if color else str(x)\n",
        "\n",
        "        #     return _inner\n",
        "\n",
        "        # match = test_out[0].argmax(-1)[:-1] != test_input[0, 1:]\n",
        "        # np.set_printoptions(formatter={\"all\": color_when_true(match)})\n",
        "        # print(\"ALL\")\n",
        "        # print(test_out[0].argmax(-1)[:-1].cpu().numpy())\n",
        "        # np.set_printoptions(formatter={\"all\": color_when_true(match)})\n",
        "        # print(test_input[0, 1:].cpu().numpy())\n",
        "        # np.set_printoptions(formatter={\"all\": color_when_true(match)})\n",
        "        # print(test_out[0].argmax(-1)[:-1] == test_input[0, 1:])\n",
        "        # np.set_printoptions()\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fE8Z_ll7GgqB"
      },
      "outputs": [],
      "source": [
        "def animate(x: List[np.ndarray], head_idx: int, fix_scale: bool = False, title: str = None):\n",
        "    \"\"\"Plot `x` as an animation.\n",
        "\n",
        "    Args:\n",
        "        x (List[np.ndarray]): _description_\n",
        "        head_idx (int): What head index to look at\n",
        "        fix_scale (bool, optional): Whether to pin the heatmap to the max range over all frames. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    # x = [epoch, head, x, y]\n",
        "    to_plot = np.stack(x)[:, head_idx, ...]\n",
        "    zmin = to_plot.min() if fix_scale else None\n",
        "    zmax = to_plot.max() if fix_scale else None\n",
        "    fig = px.imshow(\n",
        "        to_plot,\n",
        "        animation_frame=0,\n",
        "        title=title,\n",
        "        zmin=zmin,\n",
        "        zmax=zmax,\n",
        "        color_continuous_scale=\"RdBu\",\n",
        "        color_continuous_midpoint=0,\n",
        "    )\n",
        "    # Transition every 50ms\n",
        "    fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 50\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEjWbWGaGpGp",
        "outputId": "2e0d8fb6-e599-40d2-82a6-ee2f576da134"
      },
      "outputs": [],
      "source": [
        "def make_model_and_optimizer(cfg: EasyTransformerConfig) -> Tuple[EasyTransformer, t.optim.Optimizer]:\n",
        "    model = EasyTransformer(cfg)\n",
        "    model.to(device)\n",
        "\n",
        "    optim = t.optim.AdamW(model.parameters(), lr=train_cfg[\"lr\"])\n",
        "    return model, optim\n",
        "\n",
        "def checkpoint(model: t.nn.Module, optim: t.optim.Optimizer, path: str) -> None:\n",
        "    t.save(\n",
        "        {\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optim.state_dict(),\n",
        "        },\n",
        "        path,\n",
        "    )\n",
        "\n",
        "\n",
        "def load(config: EasyTransformerConfig, path: str) -> Tuple[t.nn.Module, t.optim.Optimizer]:\n",
        "    model, optim = make_model_and_optimizer(config)\n",
        "\n",
        "    checkpoint = t.load(path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    return model, optim\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model, optim = make_model_and_optimizer(cfg)\n",
        "\n",
        "# scheduler = t.optim.lr_scheduler.MultiStepLR(optim, milestones=[1000, 1500], gamma=0.1)\n",
        "scheduler = t.optim.lr_scheduler.ConstantLR(optim, factor=1)  # Easy no-op schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "n1XaYn9sGi2K",
        "outputId": "b03b9067-2422-4742-9d06-252e7a26b991"
      },
      "outputs": [],
      "source": [
        "# Out of function for now so I can get variables\n",
        "log = True\n",
        "num_epochs = 2000\n",
        "resume_run = False\n",
        "if True:\n",
        "    # def train(model, num_epochs: int = 2000, log: bool = True, resume_run: bool = False):\n",
        "    if log and not resume_run:\n",
        "        # Save off both our model config and train config\n",
        "        cfg_to_upload = dataclasses.asdict(cfg)\n",
        "        cfg_to_upload.update(train_cfg)\n",
        "        wandb.init(project=\"alex_resumes_overparameterizing\", config=cfg_to_upload)\n",
        "\n",
        "    batch_size = train_cfg[\"batch_size\"]\n",
        "\n",
        "    start_time = time.time()\n",
        "    tot_tokens = 0\n",
        "\n",
        "    w_q = []\n",
        "    w_k = []\n",
        "    w_o_0 = []\n",
        "    w_v_0 = []\n",
        "    w_o_1 = []\n",
        "    w_v_1 = []\n",
        "    w_qk = []\n",
        "    w_ov_0 = []\n",
        "    w_ov_1 = []\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        tokens = generate_data(batch_size, cfg.n_ctx, cfg.d_vocab)\n",
        "        # From that time I tried totally random data\n",
        "        # tokens = t.randint(1, cfg[\"d_vocab\"], (batch_size, cfg[\"n_ctx\"]))\n",
        "        tokens = tokens.to(device)\n",
        "\n",
        "        out = model(tokens)\n",
        "\n",
        "        flat_out = rearrange(out[:, :-1], \"b s ... -> (b s) ...\")\n",
        "        flat_tokens = rearrange(tokens[:, 1:], \"b s ... -> (b s) ...\")\n",
        "        l = F.cross_entropy(flat_out, flat_tokens)\n",
        "        l.backward()\n",
        "\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        comp_scores = {}\n",
        "        with t.no_grad():\n",
        "            # In-lined now to get access to everything\n",
        "            # q_comp, k_comp, v_comp = get_comp_scores(model)\n",
        "            W_O_0 = model.blocks[0].attn.W_O\n",
        "            W_V_0 = model.blocks[0].attn.W_V\n",
        "            W_OV_0 = t.einsum(\"imh,ihM->imM\", W_O_0, W_V_0)\n",
        "            W_Q = model.blocks[1].attn.W_Q\n",
        "            W_K = model.blocks[1].attn.W_K\n",
        "            W_V_1 = model.blocks[1].attn.W_V\n",
        "            W_O_1 = model.blocks[1].attn.W_O\n",
        "            W_QK = t.einsum(\"ihm,ihM->imM\", W_Q, W_K)\n",
        "            W_OV_1 = t.einsum(\"imh,ihM->imM\", W_O_1, W_V_1)\n",
        "\n",
        "            w_q.append(W_Q.detach().cpu().numpy())\n",
        "            w_k.append(W_K.detach().cpu().numpy())\n",
        "            w_o_0.append(W_O_0.detach().cpu().numpy())\n",
        "            w_v_0.append(W_V_0.detach().cpu().numpy())\n",
        "            w_o_1.append(W_O_1.detach().cpu().numpy())\n",
        "            w_v_1.append(W_V_1.detach().cpu().numpy())\n",
        "            w_qk.append(W_QK.detach().cpu().numpy())\n",
        "            w_ov_0.append(W_OV_0.detach().cpu().numpy())\n",
        "            w_ov_1.append(W_OV_1.detach().cpu().numpy())\n",
        "\n",
        "            layer0 = model.blocks[0]\n",
        "            layer1 = model.blocks[1]\n",
        "            left = FactoredMatrix(layer0.attn.W_V, layer0.attn.W_O)\n",
        "            qk = FactoredMatrix(layer1.attn.W_Q, layer1.attn.W_K.transpose(-2, -1))\n",
        "            q_comp = composition_scores(left, qk)\n",
        "            k_comp = composition_scores(left, qk.T)\n",
        "            v_comp = composition_scores(left, FactoredMatrix(layer1.attn.W_V, layer1.attn.W_O))\n",
        "\n",
        "            for to_head in range(len(q_comp)):\n",
        "                for from_head in range(len(q_comp[to_head])):\n",
        "                    comp_scores[f\"q_L1H{from_head}->L0H{to_head}\"] = q_comp[to_head][from_head]\n",
        "                    comp_scores[f\"k_L1H{from_head}->L0H{to_head}\"] = k_comp[to_head][from_head]\n",
        "                    comp_scores[f\"v_L1H{from_head}->L0H{to_head}\"] = v_comp[to_head][from_head]\n",
        "\n",
        "        test_loss = get_test_loss(model, 100) if epoch % 10 == 0 else None\n",
        "\n",
        "        tot_tokens += tokens.nelement()\n",
        "        if log:\n",
        "            wandb.log(\n",
        "                dict(\n",
        "                    train_loss=l,\n",
        "                    test_loss=test_loss,\n",
        "                    elapsed=time.time() - start_time,\n",
        "                    tokens=tot_tokens,\n",
        "                    **comp_scores,\n",
        "                )\n",
        "            )\n",
        "            # Could do this, but the scrobbler on wandb kinda sucks\n",
        "            if epoch % 10 == 0 and False:\n",
        "                head_idx = 0\n",
        "                to_plot = w_q[-1][head_idx]\n",
        "                fig = px.imshow(\n",
        "                    to_plot,\n",
        "                    title=\"test upload\",\n",
        "                    color_continuous_scale=\"RdBu\",\n",
        "                    color_continuous_midpoint=0,\n",
        "                )\n",
        "                wandb.log({\"w_qk_test\": fig})\n",
        "\n",
        "    if log:\n",
        "        # Try to use up the wandb free tier as fast as possible with these 5MB HTML files\n",
        "        # Uploading the plotly plots directly doesn't work for animations. See:\n",
        "        # https://github.com/wandb/wandb/issues/2014\n",
        "        # https://github.com/wandb/wandb/issues/2191\n",
        "        wandb.log({\"W_QK_0\": wandb.Html(plotly.io.to_html(animate(w_qk[::50], head_idx=0, title=\"W_QK L1H0\")))})\n",
        "        wandb.log({\"W_QK_1\": wandb.Html(plotly.io.to_html(animate(w_qk[::50], head_idx=1, title=\"W_QK L1H1\")))})\n",
        "\n",
        "    if log:\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVsh1FrpGhT1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict(model, tokens: List[int]):\n",
        "    return model(t.tensor([tokens])).argmax(-1)[:, -1]\n",
        "\n",
        "\n",
        "def line_up_induction(model, tokens: List[int]):\n",
        "    ret = model(t.tensor([tokens])).argmax(-1)\n",
        "    print(\"actual:\", \"\\t\".join(str(i) for i in np.array(tokens)[1:]))\n",
        "    print(\"pred:  \", \"\\t\".join(str(i) for i in ret[0].cpu().numpy()[:-1]))\n",
        "\n",
        "\n",
        "# TODO: Want to extra individual heads later\n",
        "def copy_layer(layer: int, from_model: t.nn.Module, to_model: t.nn.Module, freeze: bool = False) -> t.nn.Module:\n",
        "    from_dict = from_model.state_dict()\n",
        "    old_to_dict = to_model.state_dict()\n",
        "    match = f\"blocks.{layer}\"\n",
        "    patched_dict = {k: (from_dict[k] if match in k else v) for k, v in old_to_dict.items()}\n",
        "    to_model.load_state_dict(patched_dict)\n",
        "    if freeze:\n",
        "        for name, p in to_model.named_parameters():\n",
        "            if match in name:\n",
        "                p.requires_grad = False\n",
        "    return to_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 ('mlenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ba6ce4b7428af1c71bae25cd2974245e6949953152d40892f946abaf3f0f875e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
